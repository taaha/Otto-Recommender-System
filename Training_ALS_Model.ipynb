{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf88b14",
   "metadata": {},
   "source": [
    "### Reading Parquet file into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64821ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the list of files in the data directory\n",
    "file_list = [f for f in os.listdir('data/train_parquet') if f.endswith('.parquet')]\n",
    "\n",
    "# Prepend the directory path to each file name\n",
    "file_list = [os.path.join('data/train_parquet', f) for f in file_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc455e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128cbfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:35:27 WARN Utils: Your hostname, darth-Vig800S resolves to a loopback address: 127.0.1.1; using 192.168.100.39 instead (on interface eno1)\n",
      "23/01/11 22:35:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:35:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 146/146 [00:12<00:00, 11.80it/s]\n",
      "100%|████████████████████████████████████████| 145/145 [00:00<00:00, 157.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.driver.memory', '24g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through the list of Parquet files\n",
    "for file in tqdm(file_list):\n",
    "    # Read the Parquet file into a Spark DataFrame\n",
    "    df = spark.read.parquet(file)\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the list\n",
    "#df = df_list[0].union(df_list[1:])\n",
    "df = df_list[0]\n",
    "\n",
    "# Iterate through the rest of the DataFrames in the list\n",
    "for i in tqdm(range(1, len(df_list))):\n",
    "    # Union the DataFrame with the next DataFrame in the list\n",
    "    df = df.union(df_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87efd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+\n",
      "|session|    aid|type|\n",
      "+-------+-------+----+\n",
      "| 100000|1498214|   1|\n",
      "| 100000|1617298|   1|\n",
      "| 100000|1617298|   3|\n",
      "| 100000|1820189|   1|\n",
      "| 100000|1619534|   1|\n",
      "| 100000|  22770|   1|\n",
      "| 100000|  22770|   1|\n",
      "| 100000|  22770|   3|\n",
      "| 100000| 339965|   1|\n",
      "| 100000| 339965|   3|\n",
      "| 100000| 339965|   5|\n",
      "| 100000|  22770|   5|\n",
      "| 100000| 339965|   1|\n",
      "| 100000| 339965|   1|\n",
      "| 100000| 710289|   1|\n",
      "| 100001|1104009|   1|\n",
      "| 100001|1196408|   1|\n",
      "| 100001| 822736|   1|\n",
      "| 100001| 791744|   1|\n",
      "| 100001| 822736|   1|\n",
      "+-------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a1a44",
   "metadata": {},
   "source": [
    "# Fitting ALS on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568a8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:35:52 WARN DAGScheduler: Broadcasting large task binary with size 1416.9 KiB\n",
      "23/01/11 22:35:52 WARN DAGScheduler: Broadcasting large task binary with size 1416.9 KiB\n",
      "23/01/11 22:35:53 WARN DAGScheduler: Broadcasting large task binary with size 1419.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:====================================================>(325 + 1) / 326]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:36:23 WARN DAGScheduler: Broadcasting large task binary with size 1420.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:====================================================>(324 + 2) / 326]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:36:49 WARN DAGScheduler: Broadcasting large task binary with size 1422.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:37:14 WARN DAGScheduler: Broadcasting large task binary with size 1420.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 154:====================================================>(325 + 1) / 326]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1422.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:38:59 WARN DAGScheduler: Broadcasting large task binary with size 1423.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:39:26 WARN DAGScheduler: Broadcasting large task binary with size 1426.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:40:20 WARN DAGScheduler: Broadcasting large task binary with size 1427.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:41:55 WARN DAGScheduler: Broadcasting large task binary with size 1428.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:42:56 WARN DAGScheduler: Broadcasting large task binary with size 1430.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:44:24 WARN DAGScheduler: Broadcasting large task binary with size 1431.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:45:19 WARN DAGScheduler: Broadcasting large task binary with size 1432.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 165:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:46:48 WARN DAGScheduler: Broadcasting large task binary with size 1434.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 166:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:47:48 WARN DAGScheduler: Broadcasting large task binary with size 1435.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:49:16 WARN DAGScheduler: Broadcasting large task binary with size 1437.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 168:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:50:07 WARN DAGScheduler: Broadcasting large task binary with size 1439.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:51:13 WARN DAGScheduler: Broadcasting large task binary with size 1437.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the ALS model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"session\", itemCol=\"aid\", ratingCol=\"type\")\n",
    "model = als.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ed6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#userRecs = model.recommendForAllUsers(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b11993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff2ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:09 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "23/01/11 22:52:09 WARN DAGScheduler: Broadcasting large task binary with size 1450.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:22 WARN DAGScheduler: Broadcasting large task binary with size 1840.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 214:=============================================>         (10 + 2) / 12]\r",
      "\r",
      "[Stage 214:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:24 WARN DAGScheduler: Broadcasting large task binary with size 1854.1 KiB\n",
      "23/01/11 22:52:24 WARN DAGScheduler: Broadcasting large task binary with size 1854.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+----------+\n",
      "| session|  aid|type|prediction|\n",
      "+--------+-----+----+----------+\n",
      "|12899779|59625|   1|0.99176383|\n",
      "+--------+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make recommendations for a user\n",
    "recommendations = model.transform(df.filter(df.session == 12899779))\n",
    "\n",
    "# Show the recommendations\n",
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4161e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:28 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "23/01/11 22:52:28 WARN DAGScheduler: Broadcasting large task binary with size 1450.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 266:=======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:39 WARN DAGScheduler: Broadcasting large task binary with size 1825.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 270:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:52:40 WARN DAGScheduler: Broadcasting large task binary with size 1839.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 288:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+----------+\n",
      "| session|  aid|type|prediction|\n",
      "+--------+-----+----+----------+\n",
      "|12899779|59625|   1|0.99176383|\n",
      "+--------+-----+----+----------+\n",
      "\n",
      "CPU times: user 28.4 ms, sys: 3.82 ms, total: 32.2 ms\n",
      "Wall time: 18.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "rec=recommendations.sort(desc(\"prediction\")).show(10)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b17b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd6a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e3ea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:53:19 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "23/01/11 22:53:19 WARN DAGScheduler: Broadcasting large task binary with size 1450.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 303:============>   (8 + 2) / 10][Stage 304:==============> (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:53:26 WARN DAGScheduler: Broadcasting large task binary with size 1811.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 320:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:53:26 WARN DAGScheduler: Broadcasting large task binary with size 1825.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99176383]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# select only the rating column\n",
    "ratings_df = recommendations.select(\"prediction\")\n",
    "\n",
    "# sort the ratings in descending order\n",
    "ratings_df = ratings_df.sort(desc(\"prediction\"))\n",
    "\n",
    "# convert to pandas dataframe and get only top 10\n",
    "ratings_df = ratings_df.limit(10).toPandas()\n",
    "\n",
    "# Get the values of the pandas DataFrame as a numpy array\n",
    "numpy_array = ratings_df.values\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a884bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1671803it [00:36, 45942.46it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "session_list = []\n",
    "\n",
    "with open(\"data/test.jsonl\", \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        data = json.loads(line)\n",
    "        session_list.append(data[\"session\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "199da00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12899779,\n",
       " 12899780,\n",
       " 12899781,\n",
       " 12899782,\n",
       " 12899783,\n",
       " 12899784,\n",
       " 12899785,\n",
       " 12899786,\n",
       " 12899787,\n",
       " 12899788]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8f2b5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671803"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(session_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957890f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12899779,\n",
       " 12899780,\n",
       " 12899781,\n",
       " 12899782,\n",
       " 12899783,\n",
       " 12899784,\n",
       " 12899785,\n",
       " 12899786,\n",
       " 12899787,\n",
       " 12899788]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=session_list[0:10]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc6598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('data/mid_file1.jsonl', 'w') as f:\n",
    "#     for x in tqdm(temp):\n",
    "#         recommendations = model.transform(df.filter(df.session == x))\n",
    "#         ratings_df = recommendations.select(\"prediction\")\n",
    "#         # sort the ratings in descending order\n",
    "#         ratings_df = ratings_df.sort(desc(\"prediction\"))\n",
    "\n",
    "#         # convert to pandas dataframe and get only top 10\n",
    "#         ratings_df = ratings_df.limit(10).toPandas()\n",
    "\n",
    "#         # Get the values of the pandas DataFrame as a numpy array\n",
    "#         numpy_array = ratings_df.values\n",
    "#         print(numpy_array)\n",
    "        \n",
    "#         string_values = [str(x1) for x1 in numpy_array]\n",
    "#         aids_str = ' '.join(string_values)\n",
    "        \n",
    "#         json_record = json.dumps(aids_str, ensure_ascii=False)\n",
    "#         f.write(str(x) + json_record + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2032882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1650.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 22:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1649.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"data/als_model1\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61402736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
